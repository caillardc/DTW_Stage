{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ec9462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Conv1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from CLayer.CLayer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "388d1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import csv\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_classes = int(np.amax(labels_dense) + 1)\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "    def __init__(self, images, labels, one_hot=False):\n",
    "        #images_2 is series\n",
    "        assert images.shape[0] == labels.shape[0], ('images_1.shape: %s labels_1.shape: %s' % (images.shape, labels.shape))\n",
    "        self._num_examples = images.shape[0]\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, rows*columns] (assuming depth == 1)\n",
    "        # assert images.shape[3] == 1\n",
    "        #images = images.reshape(images.shape[0], images.shape[1] * images.shape[2] * images.shape[3])\n",
    "        # Convert from [0, 255] -> [-1.0, 1.0].\n",
    "        images = images.astype(np.float32)\n",
    "        images = np.multiply(images, 1.0 / 127.5) - 1.\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            print(\"epoch \" + str(self._epochs_completed))\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._images[start:end], self._labels[start:end]\n",
    "\n",
    "\n",
    "def load_data_from_pickle(data_file, label_file, image_shape):\n",
    "    import pickle\n",
    "    print(data_file)\n",
    "    output = open(data_file, 'rb')\n",
    "    labels = pickle.load(output)\n",
    "    images = pickle.load(output)\n",
    "    output.close()\n",
    "    images = np.reshape(images, (np.shape(labels)[0], image_shape[0], image_shape[1], image_shape[2]))\n",
    "    return images, labels\n",
    "\n",
    "def load_data(data_file, label_file, image_shape, onehot):\n",
    "    print(data_file)\n",
    "    images = np.genfromtxt(data_file, delimiter=' ')\n",
    "    labels = np.genfromtxt(label_file, usecols=(1), delimiter=' ')\n",
    "    if onehot:\n",
    "       labels = dense_to_one_hot(labels.astype(int))\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def load_data_from_file(data_file, label_file, image_shape, onehot):\n",
    "    print(data_file)\n",
    "    labelsall = np.genfromtxt(label_file, delimiter=' ', dtype=None)\n",
    "    labelsshape = np.shape(labelsall)\n",
    "    \n",
    "    images = np.zeros((labelsshape[0], image_shape[0], image_shape[1], image_shape[2]))\n",
    "    labels = np.zeros((labelsshape[0]))\n",
    "    count = 0\n",
    "    for line in labelsall:\n",
    "       labels[count] = line[1]\n",
    "       imagefile = im.open(data_file + line[0].decode(\"utf-8\"))\n",
    "       imagefile = imagefile.convert('RGB')\n",
    "       images[count] = np.array(imagefile)\n",
    "       imagefile.close()\n",
    "       if count % 1000 == 0:\n",
    "           print(count)\n",
    "       count += 1\n",
    "    if onehot:\n",
    "       labels = dense_to_one_hot(labels.astype(int))\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def read_data_sets(train_file, train_label, shape, test_file=\"\", test_label=\"\", test_ratio=0.1, validation_ratio=0.0, pickle=True, boring=False, onehot=False):\n",
    "    class DataSets(object):\n",
    "        pass\n",
    "\n",
    "    data_sets = DataSets()\n",
    "\n",
    "    if (pickle):\n",
    "        train_images, train_labels = load_data_from_pickle(train_file, train_label, shape)\n",
    "        if test_file:\n",
    "            test_images, test_labels = load_data_from_pickle(test_file, test_label, shape)\n",
    "        else:\n",
    "            test_size = int(test_ratio * float(train_labels.shape[0]))\n",
    "            test_images = train_images[:test_size]\n",
    "            test_labels = train_labels[:test_size]\n",
    "            train_images = train_images[test_size:]\n",
    "            train_labels = train_labels[test_size:]\n",
    "    elif(boring):\n",
    "        train_images, train_labels = load_data_from_file(train_file, train_label, shape, onehot)\n",
    "        if test_file:\n",
    "            test_images, test_labels = load_data_from_file(test_file, test_label, shape, onehot)\n",
    "        else:\n",
    "            test_size = int(test_ratio * float(train_labels.shape[0]))\n",
    "            test_images = train_images[:test_size]\n",
    "            test_labels = train_labels[:test_size]\n",
    "            train_images = train_images[test_size:]\n",
    "            train_labels = train_labels[test_size:]\n",
    "    else:\n",
    "        train_images, train_labels = load_data(train_file, train_label, shape, onehot)\n",
    "        if test_file:\n",
    "            test_images, test_labels = load_data(test_file, test_label, shape, onehot)\n",
    "        else:\n",
    "            test_size = int(test_ratio * float(train_labels.shape[0]))\n",
    "            test_images = train_images[:test_size]\n",
    "            test_labels = train_labels[:test_size]\n",
    "            train_images = train_images[test_size:]\n",
    "            train_labels = train_labels[test_size:]\n",
    "\n",
    "    validation_size = int(validation_ratio * float(train_labels.shape[0]))\n",
    "    validation_images = train_images[:validation_size]\n",
    "    validation_labels = train_labels[:validation_size]\n",
    "\n",
    "    train_images = train_images[validation_size:]\n",
    "    train_labels = train_labels[validation_size:]\n",
    "\n",
    "    data_sets.train = DataSet(train_images, train_labels)\n",
    "    data_sets.validation = DataSet(validation_images, validation_labels)\n",
    "    data_sets.test = DataSet(test_images, test_labels)\n",
    "    \n",
    "    print(\"data loaded\")\n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c5a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Unipen/1a-re-data.txt\n",
      "data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11700, 50, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_FILE = \"../data/Unipen/1a-re-data.txt\"\n",
    "TRAINING_LABEL = \"../data/Unipen/1a-re-labels.txt\"\n",
    "IMAGE_SHAPE = (50, 2)\n",
    "data_sets = read_data_sets(TRAINING_FILE, TRAINING_LABEL, IMAGE_SHAPE, validation_ratio=0.0, pickle=False, boring=False, onehot=True)\n",
    "train_data = (data_sets.train.images.reshape((-1, 50, 2)) + 1. ) * (127.5 / 127.)   # this input_data assumes images\n",
    "train_labels = data_sets.train.labels\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c824c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11700, 50, 2)\n",
      "(11700, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "684ba533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 48, 10)            70        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                4810      \n",
      "=================================================================\n",
      "Total params: 4,880\n",
      "Trainable params: 4,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 1.4441 - accuracy: 0.6002 - val_loss: 0.9963 - val_accuracy: 0.7314\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.8695 - accuracy: 0.7565 - val_loss: 0.7724 - val_accuracy: 0.7747\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.7198 - accuracy: 0.7936 - val_loss: 0.6808 - val_accuracy: 0.8032\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.8124 - val_loss: 0.6288 - val_accuracy: 0.8046\n",
      "Epoch 5/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.5954 - accuracy: 0.8286 - val_loss: 0.5868 - val_accuracy: 0.8300\n",
      "Epoch 6/20\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.5545 - accuracy: 0.8393 - val_loss: 0.5592 - val_accuracy: 0.8371\n",
      "Epoch 7/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.5238 - accuracy: 0.8503 - val_loss: 0.5241 - val_accuracy: 0.8485\n",
      "Epoch 8/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.4949 - accuracy: 0.8572 - val_loss: 0.4995 - val_accuracy: 0.8562\n",
      "Epoch 9/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.4687 - accuracy: 0.8651 - val_loss: 0.4844 - val_accuracy: 0.8644\n",
      "Epoch 10/20\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4461 - accuracy: 0.8750 - val_loss: 0.4656 - val_accuracy: 0.8735\n",
      "Epoch 11/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.4258 - accuracy: 0.8823 - val_loss: 0.4458 - val_accuracy: 0.8758\n",
      "Epoch 12/20\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4098 - accuracy: 0.8861 - val_loss: 0.4251 - val_accuracy: 0.8801\n",
      "Epoch 13/20\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3938 - accuracy: 0.8906 - val_loss: 0.4133 - val_accuracy: 0.8855\n",
      "Epoch 14/20\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3780 - accuracy: 0.8944 - val_loss: 0.4071 - val_accuracy: 0.8889\n",
      "Epoch 15/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3665 - accuracy: 0.8989 - val_loss: 0.4025 - val_accuracy: 0.8932\n",
      "Epoch 16/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3557 - accuracy: 0.9008 - val_loss: 0.4022 - val_accuracy: 0.8881\n",
      "Epoch 17/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3454 - accuracy: 0.9045 - val_loss: 0.3767 - val_accuracy: 0.8995\n",
      "Epoch 18/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3361 - accuracy: 0.9056 - val_loss: 0.3755 - val_accuracy: 0.8963\n",
      "Epoch 19/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3300 - accuracy: 0.9094 - val_loss: 0.3667 - val_accuracy: 0.9037\n",
      "Epoch 20/20\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.3210 - accuracy: 0.9098 - val_loss: 0.3665 - val_accuracy: 0.9057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f5d01ef860>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(train_data.shape[1:]),\n",
    "    Conv1D(10,3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_data, train_labels, epochs=10, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9597d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"map/while/stack_130:0\", shape=(10,), dtype=float32)\n",
      "Tensor(\"map/TensorArrayV2Stack/TensorListStack:0\", shape=(48, 10), dtype=float32)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dwa_cnn (DWA_CNN)            (None, 48, 10)            70        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                4810      \n",
      "=================================================================\n",
      "Total params: 4,880\n",
      "Trainable params: 4,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 2328s 9s/step - loss: 1.3262 - accuracy: 0.6604 - val_loss: 0.8856 - val_accuracy: 0.7434\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 2569s 10s/step - loss: 0.8043 - accuracy: 0.7668 - val_loss: 0.7442 - val_accuracy: 0.7830\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 5377s 21s/step - loss: 0.7079 - accuracy: 0.7953 - val_loss: 0.6837 - val_accuracy: 0.7992\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 3644s 14s/step - loss: 0.6575 - accuracy: 0.8110 - val_loss: 0.6501 - val_accuracy: 0.8021\n",
      "Epoch 5/10\n",
      " 69/256 [=======>......................] - ETA: 31:47 - loss: 0.6298 - accuracy: 0.8120"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(train_data.shape[1:]),\n",
    "    DWA_CNN(10,3),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_data, train_labels, epochs=10, validation_split=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
