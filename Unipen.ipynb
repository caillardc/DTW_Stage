{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02057f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Conv1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from CLayer.CLayer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9cfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import csv\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_classes = int(np.amax(labels_dense) + 1)\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "    def __init__(self, images, labels, one_hot=False):\n",
    "        #images_2 is series\n",
    "        assert images.shape[0] == labels.shape[0], ('images_1.shape: %s labels_1.shape: %s' % (images.shape, labels.shape))\n",
    "        self._num_examples = images.shape[0]\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, rows*columns] (assuming depth == 1)\n",
    "        # assert images.shape[3] == 1\n",
    "        #images = images.reshape(images.shape[0], images.shape[1] * images.shape[2] * images.shape[3])\n",
    "        # Convert from [0, 255] -> [-1.0, 1.0].\n",
    "        images = images.astype(np.float32)\n",
    "        images = np.multiply(images, 1.0 / 127.5) - 1.\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            print(\"epoch \" + str(self._epochs_completed))\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._images[start:end], self._labels[start:end]\n",
    "\n",
    "\n",
    "def load_data_from_pickle(data_file, label_file, image_shape):\n",
    "    import pickle\n",
    "    print(data_file)\n",
    "    output = open(data_file, 'rb')\n",
    "    labels = pickle.load(output)\n",
    "    images = pickle.load(output)\n",
    "    output.close()\n",
    "    images = np.reshape(images, (np.shape(labels)[0], image_shape[0], image_shape[1], image_shape[2]))\n",
    "    return images, labels\n",
    "\n",
    "def load_data(data_file, label_file, image_shape, onehot):\n",
    "    print(data_file)\n",
    "    images = np.genfromtxt(data_file, delimiter=' ')\n",
    "    labels = np.genfromtxt(label_file, usecols=(1), delimiter=' ')\n",
    "    if onehot:\n",
    "       labels = dense_to_one_hot(labels.astype(int))\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def load_data_from_file(data_file, label_file, image_shape, onehot):\n",
    "    print(data_file)\n",
    "    labelsall = np.genfromtxt(label_file, delimiter=' ', dtype=None)\n",
    "    labelsshape = np.shape(labelsall)\n",
    "    \n",
    "    images = np.zeros((labelsshape[0], image_shape[0], image_shape[1], image_shape[2]))\n",
    "    labels = np.zeros((labelsshape[0]))\n",
    "    count = 0\n",
    "    for line in labelsall:\n",
    "       labels[count] = line[1]\n",
    "       imagefile = im.open(data_file + line[0].decode(\"utf-8\"))\n",
    "       imagefile = imagefile.convert('RGB')\n",
    "       images[count] = np.array(imagefile)\n",
    "       imagefile.close()\n",
    "       if count % 1000 == 0:\n",
    "           print(count)\n",
    "       count += 1\n",
    "    if onehot:\n",
    "       labels = dense_to_one_hot(labels.astype(int))\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def read_data_sets(train_file, train_label, shape, test_file=\"\", test_label=\"\", test_ratio=0.1, validation_ratio=0.0, pickle=True, boring=False, onehot=False):\n",
    "    class DataSets(object):\n",
    "        pass\n",
    "\n",
    "    data_sets = DataSets()\n",
    "\n",
    "    if (pickle):\n",
    "        train_images, train_labels = load_data_from_pickle(train_file, train_label, shape)\n",
    "        if test_file:\n",
    "            test_images, test_labels = load_data_from_pickle(test_file, test_label, shape)\n",
    "        else:\n",
    "            test_size = int(test_ratio * float(train_labels.shape[0]))\n",
    "            test_images = train_images[:test_size]\n",
    "            test_labels = train_labels[:test_size]\n",
    "            train_images = train_images[test_size:]\n",
    "            train_labels = train_labels[test_size:]\n",
    "    elif(boring):\n",
    "        train_images, train_labels = load_data_from_file(train_file, train_label, shape, onehot)\n",
    "        if test_file:\n",
    "            test_images, test_labels = load_data_from_file(test_file, test_label, shape, onehot)\n",
    "        else:\n",
    "            test_size = int(test_ratio * float(train_labels.shape[0]))\n",
    "            test_images = train_images[:test_size]\n",
    "            test_labels = train_labels[:test_size]\n",
    "            train_images = train_images[test_size:]\n",
    "            train_labels = train_labels[test_size:]\n",
    "    else:\n",
    "        train_images, train_labels = load_data(train_file, train_label, shape, onehot)\n",
    "        if test_file:\n",
    "            test_images, test_labels = load_data(test_file, test_label, shape, onehot)\n",
    "        else:\n",
    "            test_size = int(test_ratio * float(train_labels.shape[0]))\n",
    "            test_images = train_images[:test_size]\n",
    "            test_labels = train_labels[:test_size]\n",
    "            train_images = train_images[test_size:]\n",
    "            train_labels = train_labels[test_size:]\n",
    "\n",
    "    validation_size = int(validation_ratio * float(train_labels.shape[0]))\n",
    "    validation_images = train_images[:validation_size]\n",
    "    validation_labels = train_labels[:validation_size]\n",
    "\n",
    "    train_images = train_images[validation_size:]\n",
    "    train_labels = train_labels[validation_size:]\n",
    "\n",
    "    data_sets.train = DataSet(train_images, train_labels)\n",
    "    data_sets.validation = DataSet(validation_images, validation_labels)\n",
    "    data_sets.test = DataSet(test_images, test_labels)\n",
    "    \n",
    "    print(\"data loaded\")\n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329439d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Unipen/1a-re-data.txt\n",
      "data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11700, 50, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_FILE = \"../data/Unipen/1a-re-data.txt\"\n",
    "TRAINING_LABEL = \"../data/Unipen/1a-re-labels.txt\"\n",
    "IMAGE_SHAPE = (50, 2)\n",
    "data_sets = read_data_sets(TRAINING_FILE, TRAINING_LABEL, IMAGE_SHAPE, validation_ratio=0.0, pickle=False, boring=False, onehot=True)\n",
    "train_data = (data_sets.train.images.reshape((-1, 50, 2)) + 1. ) * (127.5 / 127.)   # this input_data assumes images\n",
    "train_labels = data_sets.train.labels\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e3b1e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 2)]           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 48, 10)            70        \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 480)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                4810      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,880\n",
      "Trainable params: 4,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 1.4886 - accuracy: 0.6021\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.8638 - accuracy: 0.7602\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.7188 - accuracy: 0.7934\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.8103\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.6068 - accuracy: 0.8242\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.5722 - accuracy: 0.8344\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.5420 - accuracy: 0.8453\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.8536\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.4907 - accuracy: 0.8603\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.4684 - accuracy: 0.8665\n",
      "CPU times: total: 8.66 s\n",
      "Wall time: 5.95 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9b2b059d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "input_layer =  Input(train_data.shape[1:])\n",
    "conv_layer = Conv1D(10,3, activation='relu')(input_layer)\n",
    "flatten = Flatten()(conv_layer)\n",
    "output_layer = Dense(10, activation='softmax')(flatten)   \n",
    "model = Model(input_layer,output_layer)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels,batch_size=64,epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768e5ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cnn1d (CNN1D)               (None, 48, 10)            70        \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 480)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                4810      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,880\n",
      "Trainable params: 4,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "183/183 [==============================] - 10s 7ms/step - loss: 1.4886 - accuracy: 0.6021\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.8638 - accuracy: 0.7602\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.7188 - accuracy: 0.7934\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.6515 - accuracy: 0.8103\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.6068 - accuracy: 0.8242\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.5722 - accuracy: 0.8344\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.5420 - accuracy: 0.8453\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.5152 - accuracy: 0.8536\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.4907 - accuracy: 0.8603\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.4684 - accuracy: 0.8665\n",
      "CPU times: total: 1min 12s\n",
      "Wall time: 23.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9b2e55190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(train_data.shape[1:]),\n",
    "    CNN1D(10,3),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels,batch_size=64,epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ac9447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dwa_cnn_np (DWA_CNN_np)     (None, 48, 10)            70        \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 480)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                4810      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,880\n",
      "Trainable params: 4,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "256/256 [==============================] - 1794s 7s/step - loss: 1.4441 - accuracy: 0.6002 - val_loss: 0.9963 - val_accuracy: 0.7314\n",
      "CPU times: total: 33min 53s\n",
      "Wall time: 29min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9b2ec62b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(train_data.shape[1:]),\n",
    "    DWA_CNN_np(10,3),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_data, train_labels, epochs=1, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd57a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dwa_cnn (DWA_CNN)           (None, 48, 10)            70        \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 480)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                4810      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,880\n",
      "Trainable params: 4,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "  6/256 [..............................] - ETA: 47:07 - loss: 2.3413 - accuracy: 0.1302"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(train_data.shape[1:]),\n",
    "    DWA_CNN(10,3),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_data, train_labels, epochs=1, validation_split=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StageEnv",
   "language": "python",
   "name": "stageenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
