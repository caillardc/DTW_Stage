{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be1e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Conv1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from CLayer.CLayer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5170872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from random import shuffle\n",
    "\n",
    "def get_filepaths(mainfolder):\n",
    "    \"\"\"\n",
    "    Searches a folder for all unique files and compile a dictionary of their paths.\n",
    "    Parameters\n",
    "    --------------\n",
    "    mainfolder: the filepath for the folder containing the data\n",
    "    Returns\n",
    "    --------------\n",
    "    training_filepaths: file paths to be used for training\n",
    "    testing_filepaths:  file paths to be used for testing\n",
    "    \"\"\"\n",
    "    training_filepaths = {}\n",
    "    testing_filepaths  = {}\n",
    "    folders = os.listdir(mainfolder)\n",
    "    for folder in folders:\n",
    "        fpath = mainfolder + \"/\" + folder\n",
    "        if os.path.isdir(fpath) and \"MODEL\" in folder:\n",
    "            filenames = os.listdir(fpath)\n",
    "            for filename in filenames[:int(round(0.8*len(filenames)))]:\n",
    "                fullpath = fpath + \"/\" + filename\n",
    "                training_filepaths[fullpath] = folder\n",
    "            for filename1 in filenames[int(round(0.8*len(filenames))):]:\n",
    "                fullpath1 = fpath + \"/\" + filename1\n",
    "                testing_filepaths[fullpath1] = folder\n",
    "    return training_filepaths, testing_filepaths\n",
    "\n",
    "def get_labels(mainfolder):\n",
    "    \"\"\" Creates a dictionary of labels for each unique type of motion \"\"\"\n",
    "    labels = {}\n",
    "    label = 0\n",
    "    for folder in os.listdir(mainfolder):\n",
    "        fpath = mainfolder + \"/\" + folder\n",
    "        if os.path.isdir(fpath) and \"MODEL\" in folder:\n",
    "            labels[folder] = label\n",
    "            label += 1\n",
    "    return labels\n",
    "\n",
    "def get_data(fp, labels, folders, norm, std, center):\n",
    "    \"\"\"\n",
    "    Creates a dataframe for the data in the filepath and creates a one-hot\n",
    "    encoding of the file's label\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filepath_or_buffer=fp, sep=' ', names = [\"X\", \"Y\", \"Z\"])\n",
    "    if norm and not std:\n",
    "        normed_data = norm_data(data)\n",
    "    elif std and not norm:\n",
    "        stdized_data = std_data(data)\n",
    "    elif center and not norm and not std:\n",
    "        cent_data = subtract_mean(data)\n",
    "\n",
    "    one_hot = np.zeros(7)\n",
    "    file_dir = folders[fp]\n",
    "    label = labels[file_dir]\n",
    "    one_hot[label] = 1\n",
    "    return normed_data, one_hot, label\n",
    "\n",
    "# Normalizes the data by removing the mean\n",
    "\n",
    "def subtract_mean(input_data):\n",
    "    # Subtract the mean along each column\n",
    "    centered_data = input_data - input_data.mean()\n",
    "    return centered_data\n",
    "\n",
    "\n",
    "def norm_data(data):\n",
    "    \"\"\"\n",
    "    Normalizes the data.\n",
    "    For normalizing each entry, y = (x - min)/(max - min)\n",
    "    \"\"\"\n",
    "    c_data = subtract_mean(data)\n",
    "    mms = MinMaxScaler()\n",
    "    mms.fit(c_data)\n",
    "    n_data = mms.transform(c_data)\n",
    "    return n_data\n",
    "\n",
    "def standardize(data):\n",
    "    c_data = subtract_mean(data)\n",
    "    std_data = c_data/ pd.std(c_data)\n",
    "    return std_data\n",
    "\n",
    "def vectorize(normed):\n",
    "    \"\"\"\n",
    "    Uses a sliding window to create a list of (randomly-ordered) 300-timestep\n",
    "    sublists for each feature.\n",
    "    \"\"\"\n",
    "    sequences = [normed[i:i+150] for i in range(len(normed)-150)]\n",
    "    shuffle(sequences)\n",
    "    sequences = np.array(sequences)\n",
    "    return sequences\n",
    "\n",
    "def build_inputs(files_list, accel_labels, file_label_dict, norm_bool, std_bool, center_bool):\n",
    "    X_seq    = []\n",
    "    y_seq    = []\n",
    "    labels = []\n",
    "    for path in files_list:\n",
    "        normed_data, target, target_label = get_data(path, accel_labels, file_label_dict, norm_bool, std_bool, center_bool)\n",
    "        input_list = vectorize(normed_data)\n",
    "        for inputs in range(len(input_list)):\n",
    "            X_seq.append(input_list[inputs])\n",
    "            y_seq.append(list(target))\n",
    "            labels.append(target_label)\n",
    "    X_ = np.array(X_seq)\n",
    "    y_ = np.array(y_seq)\n",
    "    return X_, y_, labels\n",
    "\n",
    "mainpath = \"../data/HMP_Dataset\"\n",
    "\n",
    "\n",
    "activity_labels                  = get_labels(mainpath)\n",
    "training_dict, testing_dict      = get_filepaths(mainpath)\n",
    "training_files                   = list(training_dict.keys())\n",
    "testing_files                    = list(testing_dict.keys())\n",
    "\n",
    "    # build training inputs and labels\n",
    "X_train, y_train, train_labels = build_inputs(\n",
    "    training_files,\n",
    "    activity_labels,\n",
    "    training_dict,\n",
    "    True, False, False)\n",
    "\n",
    "\n",
    "X_test, y_test, test_labels = build_inputs(\n",
    "    testing_files,\n",
    "    activity_labels,\n",
    "    testing_dict,\n",
    "    True, False, False)\n",
    "\n",
    "shuffle = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffle]\n",
    "y_train = y_train[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0d4dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cn_n1d (CNN1D)               (None, 147, 10)           130       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1470)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 10297     \n",
      "=================================================================\n",
      "Total params: 10,427\n",
      "Trainable params: 10,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 63s 213ms/step - loss: 0.7011 - accuracy: 0.7675 - val_loss: 0.6211 - val_accuracy: 0.8367\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 63s 213ms/step - loss: 0.3308 - accuracy: 0.8903 - val_loss: 0.3862 - val_accuracy: 0.8793\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 61s 209ms/step - loss: 0.2448 - accuracy: 0.9150 - val_loss: 0.3860 - val_accuracy: 0.8699\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 64s 216ms/step - loss: 0.1979 - accuracy: 0.9325 - val_loss: 0.3438 - val_accuracy: 0.8567\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 63s 213ms/step - loss: 0.1662 - accuracy: 0.9456 - val_loss: 0.3192 - val_accuracy: 0.8797\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 61s 208ms/step - loss: 0.1429 - accuracy: 0.9539 - val_loss: 0.2396 - val_accuracy: 0.9171\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 63s 216ms/step - loss: 0.1267 - accuracy: 0.9602 - val_loss: 0.2608 - val_accuracy: 0.9009\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 64s 217ms/step - loss: 0.1133 - accuracy: 0.9651 - val_loss: 0.2494 - val_accuracy: 0.9150\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 64s 216ms/step - loss: 0.1012 - accuracy: 0.9699 - val_loss: 0.2693 - val_accuracy: 0.8788\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 61s 209ms/step - loss: 0.0926 - accuracy: 0.9737 - val_loss: 0.2865 - val_accuracy: 0.9069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec2a567b70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(X_train.shape[1:]),\n",
    "    CNN1D(10, 4),\n",
    "    Flatten(),\n",
    "    Dense(7, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(X_train, y_train, epochs=10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(X_train.shape[1:]),\n",
    "    Conv1D(10,3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(7, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(X_train, y_train, epochs=10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d745fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(X_train.shape[1:]),\n",
    "    DWA_CNN(10, 3),\n",
    "    Flatten(),\n",
    "    Dense(7, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(X_train, y_train, epochs=10, validation_data = (X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
