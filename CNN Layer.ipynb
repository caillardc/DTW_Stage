{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Conv1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(x, y):\n",
    "    dist = tf.sqrt(float(tf.reduce_sum(tf.square(x - y))))\n",
    "    return dist\n",
    "\n",
    "def DTW_TF(S, S1, d=euclideanDistance):\n",
    "    cost_matrix = []\n",
    "    cost_matrix.append([0, *([1234567891011] * S1.shape[0])])\n",
    "    for i in range(1,S.shape[0]+1):\n",
    "        sub_cost_j = [1234567891011]\n",
    "        for j in range(1, S1.shape[0]+1):\n",
    "            dst = d(S[i-1], S1[j-1])\n",
    "            mat_dt = tf.stack([\n",
    "            dst + sub_cost_j[j-1],\n",
    "            dst + cost_matrix[i-1][j-1],\n",
    "            dst + cost_matrix[i-1][j]\n",
    "            ])\n",
    "            sub_cost_j.append(tf.reduce_min(mat_dt))\n",
    "        cost_matrix.append(tf.stack(sub_cost_j))\n",
    "    return DTW_minimal_path(tf.stack(cost_matrix))\n",
    "\n",
    "def DTW_minimal_path(cost_mat):\n",
    "    i = tf.constant(cost_mat.shape[0]) - 1\n",
    "    j = tf.constant(cost_mat.shape[1]) - 1\n",
    "    cost = cost_mat[i, j]\n",
    "    compteur = 0\n",
    "    path_input = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "    path_input = path_input.write(compteur, i-1)\n",
    "\n",
    "    path_output = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "    path_output = path_output.write(compteur, j-1)\n",
    "    while tf.greater(i, 0) and tf.greater(j, 0):\n",
    "        compteur += 1 \n",
    "        cost_min = tf.stack([\n",
    "            cost_mat[i-1, j-1],\n",
    "            cost_mat[i, j-1],\n",
    "            cost_mat[i-1, j]\n",
    "        ])\n",
    "        n_min = tf.math.argmin(cost_min)\n",
    "        if tf.equal(n_min,0):\n",
    "            path_input = path_input.write(compteur, i-2)\n",
    "            path_output = path_output.write(compteur, j-2)\n",
    "            i += -1\n",
    "            j += -1\n",
    "        elif tf.equal(n_min,1):\n",
    "            path_input = path_input.write(compteur, i-1)\n",
    "            path_output = path_output.write(compteur, j-2)\n",
    "            j += -1\n",
    "        elif tf.equal(n_min, 2):\n",
    "            path_input = path_input.write(compteur, i-2)\n",
    "            path_output = path_output.write(compteur, j-1)\n",
    "            i += -1\n",
    "    return path_input.stack()[:-1][::-1], path_output.stack()[:-1][::-1], cost\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 0, 1, 2, 3, 4, 4, 5, 6, 7])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 2, 2, 3, 4, 5, 5, 6])>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = [1, 3, 3, 3,2, 0, 0, 1]\n",
    "S1 = [0, 1, 3, 2, 2, 0, 1]\n",
    "\n",
    "# distance = dtw.distance(S, S1)\n",
    "# print(distance)\n",
    "# print(dtw.warping_paths(S, S1)[1])\n",
    "\n",
    "S = tf.convert_to_tensor(S)\n",
    "S1 = tf.convert_to_tensor(S1)\n",
    "\n",
    "i = 7\n",
    "j = 7\n",
    "DTW_TF(S, S1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def convolution_1D(inputs, weights):\n",
    "    weights = tf.linalg.matrix_transpose(weights)\n",
    "    final_shape = (inputs.shape[-2] - weights.shape[-1] + 1, weights.shape[0])\n",
    "    output_list = []\n",
    "        #output = tf.reshape(tf.linalg.trace(tf.linalg.matmul(weights, inputs[0,0:0+weights.shape[1]])), (1,*final_shape[2:]))\n",
    "    for j in range(0, inputs.shape[-2] - weights.shape[-1] + 1):\n",
    "        output_list.append(tf.linalg.trace(tf.linalg.matmul(inputs[j:j+weights.shape[-1]], weights)))\n",
    "    output_final = tf.stack(output_list)\n",
    "    return output_final\n",
    "\n",
    "def stride_alignment(strid_input, weights):\n",
    "        output_list = []\n",
    "        for filt in range(weights.shape[0]):\n",
    "            t_input = strid_input\n",
    "            t_weight = weights[filt]\n",
    "            path_input, path_weight, _ = DTW_TF(t_input, t_weight)\n",
    "            weights_align = tf.gather(t_weight, indices=path_weight)\n",
    "            inputs_n = tf.gather(t_input, indices=path_input)\n",
    "            weights_align = tf.linalg.matrix_transpose(weights_align)\n",
    "            output_list.append(tf.linalg.trace(tf.linalg.matmul(inputs_n, weights_align)))\n",
    "        print(tf.stack(output_list))\n",
    "        return tf.stack(output_list)\n",
    "\n",
    "@tf.function\n",
    "def conv1D_weight_alignment(inputs, weights):\n",
    "    final_shape = (inputs.shape[-2] - weights.shape[-2] + 1, weights.shape[0])\n",
    "    output_final = []\n",
    "    tensor_iter =  tf.constant([*range(0, inputs.shape[-2] - weights.shape[-2] + 1)])\n",
    "    output_final = tf.map_fn(lambda j: stride_alignment(tf.slice(inputs, (j, 0) , (weights.shape[-2:])), weights), tensor_iter, fn_output_signature=tf.float32)\n",
    "    print(output_final)\n",
    "    return output_final\n",
    "\n",
    "class CNN1D(keras.layers.Layer):\n",
    "    def __init__(self, n_filters=8, kernel_size=3):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.b = self.add_weight(shape=(n_filters,), initializer=\"zeros\", trainable=True)\n",
    "    \n",
    "    def build(self, input_shape): \n",
    "        self.w = self.add_weight(\n",
    "            shape=(self.n_filters, self.kernel_size, int(input_shape[-1])),\n",
    "            initializer=\"glorot_normal\", trainable=True\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        output = tf.map_fn(lambda inp: convolution_1D(inp, self.w) + self.b, inputs)\n",
    "        return tf.nn.relu(output)\n",
    "    \n",
    "class DWA_CNN(CNN1D):\n",
    "    def call(self, inputs):\n",
    "        output = tf.map_fn(lambda inp: conv1D_weight_alignment(inp, self.w) + self.b, inputs)\n",
    "        return tf.nn.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "randi = np.random.random((100,12, 3))\n",
    "y_train = np.random.randint(1,3, 100)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([tf.constant(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"map/while/stack_65:0\", shape=(5,), dtype=float32)\n",
      "Tensor(\"map/TensorArrayV2Stack/TensorListStack:0\", shape=(10, 5), dtype=float32)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dwa_cnn (DWA_CNN)            (None, 10, 5)             50        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 203\n",
      "Trainable params: 203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.9394 - accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.9014 - accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.8753 - accuracy: 0.5100\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.8520 - accuracy: 0.5200\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.8371 - accuracy: 0.5100\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.8259 - accuracy: 0.5100\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 2s 608ms/step - loss: 0.8102 - accuracy: 0.5400\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.7953 - accuracy: 0.5400\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 3s 625ms/step - loss: 0.7837 - accuracy: 0.5500\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 2s 598ms/step - loss: 0.7711 - accuracy: 0.5300\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.7606 - accuracy: 0.5400\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.7504 - accuracy: 0.5500\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.7439 - accuracy: 0.5500\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 2s 582ms/step - loss: 0.7383 - accuracy: 0.5400\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 2s 603ms/step - loss: 0.7305 - accuracy: 0.5400\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.7229 - accuracy: 0.5500\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.7166 - accuracy: 0.5800\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.7135 - accuracy: 0.5500\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.7106 - accuracy: 0.5600\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.7053 - accuracy: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18cc0e6a940>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(randi.shape[1:]),\n",
    "    DWA_CNN(5, 3),\n",
    "    Flatten(),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(randi, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 3)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cn_n1d_4 (CNN1D)             (None, 12, 5)             20        \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 183       \n",
      "=================================================================\n",
      "Total params: 203\n",
      "Trainable params: 203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.2446 - accuracy: 0.1400\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 1.1677 - accuracy: 0.2600\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.1220 - accuracy: 0.2900\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.0904 - accuracy: 0.4000\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.0623 - accuracy: 0.4700\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 1.0337 - accuracy: 0.4800\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1.0116 - accuracy: 0.4800\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.9897 - accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.9695 - accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.9522 - accuracy: 0.4900\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.9365 - accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.9207 - accuracy: 0.5100\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.9041 - accuracy: 0.5100\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.8891 - accuracy: 0.5100\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.8766 - accuracy: 0.5100\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.8614 - accuracy: 0.5300\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.8499 - accuracy: 0.5300\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.8371 - accuracy: 0.5400\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.8255 - accuracy: 0.5400\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.8148 - accuracy: 0.5200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ac0ca605c0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(randi.shape[1:])\n",
    "tf.random.set_seed(1234)\n",
    "model_conv = Sequential([\n",
    "    Input(randi.shape[1:]),\n",
    "    CNN1D(5,1),\n",
    "    Flatten(),\n",
    "    Dense(3, activation=\"softmax\")   \n",
    "])\n",
    "\n",
    "model_conv.summary()\n",
    "model_conv.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model_conv.fit(randi, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from random import shuffle\n",
    "\n",
    "def get_filepaths(mainfolder):\n",
    "    \"\"\"\n",
    "    Searches a folder for all unique files and compile a dictionary of their paths.\n",
    "    Parameters\n",
    "    --------------\n",
    "    mainfolder: the filepath for the folder containing the data\n",
    "    Returns\n",
    "    --------------\n",
    "    training_filepaths: file paths to be used for training\n",
    "    testing_filepaths:  file paths to be used for testing\n",
    "    \"\"\"\n",
    "    training_filepaths = {}\n",
    "    testing_filepaths  = {}\n",
    "    folders = os.listdir(mainfolder)\n",
    "    for folder in folders:\n",
    "        fpath = mainfolder + \"/\" + folder\n",
    "        if os.path.isdir(fpath) and \"MODEL\" in folder:\n",
    "            filenames = os.listdir(fpath)\n",
    "            for filename in filenames[:int(round(0.8*len(filenames)))]:\n",
    "                fullpath = fpath + \"/\" + filename\n",
    "                training_filepaths[fullpath] = folder\n",
    "            for filename1 in filenames[int(round(0.8*len(filenames))):]:\n",
    "                fullpath1 = fpath + \"/\" + filename1\n",
    "                testing_filepaths[fullpath1] = folder\n",
    "    return training_filepaths, testing_filepaths\n",
    "\n",
    "def get_labels(mainfolder):\n",
    "    \"\"\" Creates a dictionary of labels for each unique type of motion \"\"\"\n",
    "    labels = {}\n",
    "    label = 0\n",
    "    for folder in os.listdir(mainfolder):\n",
    "        fpath = mainfolder + \"/\" + folder\n",
    "        if os.path.isdir(fpath) and \"MODEL\" in folder:\n",
    "            labels[folder] = label\n",
    "            label += 1\n",
    "    return labels\n",
    "\n",
    "def get_data(fp, labels, folders, norm, std, center):\n",
    "    \"\"\"\n",
    "    Creates a dataframe for the data in the filepath and creates a one-hot\n",
    "    encoding of the file's label\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filepath_or_buffer=fp, sep=' ', names = [\"X\", \"Y\", \"Z\"])\n",
    "    if norm and not std:\n",
    "        normed_data = norm_data(data)\n",
    "    elif std and not norm:\n",
    "        stdized_data = std_data(data)\n",
    "    elif center and not norm and not std:\n",
    "        cent_data = subtract_mean(data)\n",
    "\n",
    "    one_hot = np.zeros(7)\n",
    "    file_dir = folders[fp]\n",
    "    label = labels[file_dir]\n",
    "    one_hot[label] = 1\n",
    "    return normed_data, one_hot, label\n",
    "\n",
    "# Normalizes the data by removing the mean\n",
    "\n",
    "def subtract_mean(input_data):\n",
    "    # Subtract the mean along each column\n",
    "    centered_data = input_data - input_data.mean()\n",
    "    return centered_data\n",
    "\n",
    "\n",
    "def norm_data(data):\n",
    "    \"\"\"\n",
    "    Normalizes the data.\n",
    "    For normalizing each entry, y = (x - min)/(max - min)\n",
    "    \"\"\"\n",
    "    c_data = subtract_mean(data)\n",
    "    mms = MinMaxScaler()\n",
    "    mms.fit(c_data)\n",
    "    n_data = mms.transform(c_data)\n",
    "    return n_data\n",
    "\n",
    "def standardize(data):\n",
    "    c_data = subtract_mean(data)\n",
    "    std_data = c_data/ pd.std(c_data)\n",
    "    return std_data\n",
    "\n",
    "def vectorize(normed):\n",
    "    \"\"\"\n",
    "    Uses a sliding window to create a list of (randomly-ordered) 300-timestep\n",
    "    sublists for each feature.\n",
    "    \"\"\"\n",
    "    sequences = [normed[i:i+150] for i in range(len(normed)-150)]\n",
    "    shuffle(sequences)\n",
    "    sequences = np.array(sequences)\n",
    "    return sequences\n",
    "\n",
    "def build_inputs(files_list, accel_labels, file_label_dict, norm_bool, std_bool, center_bool):\n",
    "    X_seq    = []\n",
    "    y_seq    = []\n",
    "    labels = []\n",
    "    for path in files_list:\n",
    "        normed_data, target, target_label = get_data(path, accel_labels, file_label_dict, norm_bool, std_bool, center_bool)\n",
    "        input_list = vectorize(normed_data)\n",
    "        for inputs in range(len(input_list)):\n",
    "            X_seq.append(input_list[inputs])\n",
    "            y_seq.append(list(target))\n",
    "            labels.append(target_label)\n",
    "    X_ = np.array(X_seq)\n",
    "    y_ = np.array(y_seq)\n",
    "    return X_, y_, labels\n",
    "\n",
    "mainpath = \"../data/HMP_Dataset\"\n",
    "\n",
    "\n",
    "activity_labels                  = get_labels(mainpath)\n",
    "training_dict, testing_dict      = get_filepaths(mainpath)\n",
    "training_files                   = list(training_dict.keys())\n",
    "testing_files                   = list(testing_dict.keys())\n",
    "\n",
    "    # build training inputs and labels\n",
    "X_train, y_train, train_labels = build_inputs(\n",
    "    training_files,\n",
    "    activity_labels,\n",
    "    training_dict,\n",
    "    True, False, False)\n",
    "\n",
    "\n",
    "X_test, y_test, test_labels = build_inputs(\n",
    "    testing_files,\n",
    "    activity_labels,\n",
    "    testing_dict,\n",
    "    True, False, False)\n",
    "\n",
    "shuffle = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffle]\n",
    "y_train = y_train[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9408, 150, 3) (2352, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cn_n1d_7 (CNN1D)             (None, 147, 10)           130       \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1470)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 10297     \n",
      "=================================================================\n",
      "Total params: 10,427\n",
      "Trainable params: 10,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "233/294 [======================>.......] - ETA: 9s - loss: 0.7428 - accuracy: 0.7504"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(X_train.shape[1:]),\n",
    "    CNN1D(10, 4),\n",
    "    Flatten(),\n",
    "    Dense(7, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(X_train, y_train, epochs=10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 148, 10)           100       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1480)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 10367     \n",
      "=================================================================\n",
      "Total params: 10,467\n",
      "Trainable params: 10,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.7065 - accuracy: 0.7518 - val_loss: 0.4765 - val_accuracy: 0.8486\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.3375 - accuracy: 0.8846 - val_loss: 0.3426 - val_accuracy: 0.8856\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.2520 - accuracy: 0.9068 - val_loss: 0.3361 - val_accuracy: 0.8780\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.2052 - accuracy: 0.9274 - val_loss: 0.2116 - val_accuracy: 0.9281\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.1743 - accuracy: 0.9438 - val_loss: 0.2982 - val_accuracy: 0.8805\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.1516 - accuracy: 0.9469 - val_loss: 0.3230 - val_accuracy: 0.8652\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.1342 - accuracy: 0.9562 - val_loss: 0.2759 - val_accuracy: 0.8835\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.1194 - accuracy: 0.9626 - val_loss: 0.2149 - val_accuracy: 0.9086\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.1093 - accuracy: 0.9653 - val_loss: 0.2157 - val_accuracy: 0.9048\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.0993 - accuracy: 0.9684 - val_loss: 0.2236 - val_accuracy: 0.9048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ac7fd74470>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(X_train.shape[1:]),\n",
    "    Conv1D(10,3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(7, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(X_train, y_train, epochs=10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"map/while/stack_130:0\", shape=(10,), dtype=float32)\n",
      "Tensor(\"map/TensorArrayV2Stack/TensorListStack:0\", shape=(148, 10), dtype=float32)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dwa_cnn_1 (DWA_CNN)          (None, 148, 10)           100       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1480)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 10367     \n",
      "=================================================================\n",
      "Total params: 10,467\n",
      "Trainable params: 10,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "294/294 [==============================] - 14417s 49s/step - loss: 0.6788 - accuracy: 0.7706 - val_loss: 0.5398 - val_accuracy: 0.8384\n",
      "Epoch 2/10\n",
      " 77/294 [======>.......................] - ETA: 1:32:00 - loss: 0.3889 - accuracy: 0.8685"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "    Input(X_train.shape[1:]),\n",
    "    DWA_CNN(10, 3),\n",
    "    Flatten(),\n",
    "    Dense(7, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(X_train, y_train, epochs=10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1 3]\n",
      "  [4 5]]], shape=(1, 2, 2), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = tf.constant([1,3,4,5], shape=(1,2,2))\n",
    "print(weights)\n",
    "weights.shape[:0:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
